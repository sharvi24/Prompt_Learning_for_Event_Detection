{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "718b0a6731cb4717b78a3515cb1cffc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_201f69abb3c0464c9732501450292c06",
              "IPY_MODEL_c69c92defc93420ab6b16ca6086f1f4c",
              "IPY_MODEL_6bce9241e7a849afa1afdf2476f8bbce"
            ],
            "layout": "IPY_MODEL_c85fcefb0f664b0a82dfd862cae8a30c"
          }
        },
        "201f69abb3c0464c9732501450292c06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ce497d2df14dd8973d29b11c417cb1",
            "placeholder": "​",
            "style": "IPY_MODEL_6c72be565d2d447b9b76f57050146582",
            "value": "Downloading: 100%"
          }
        },
        "c69c92defc93420ab6b16ca6086f1f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9f63bd56cb34940a972126eda1469b8",
            "max": 1199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa2a0aa5546f4c6c89b0a9057aa36ba1",
            "value": 1199
          }
        },
        "6bce9241e7a849afa1afdf2476f8bbce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b353497fe93a4e498c9757d3aa013a1b",
            "placeholder": "​",
            "style": "IPY_MODEL_7eb60db730a34e3c87af01cfd2efa544",
            "value": " 1.20k/1.20k [00:00&lt;00:00, 31.1kB/s]"
          }
        },
        "c85fcefb0f664b0a82dfd862cae8a30c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2ce497d2df14dd8973d29b11c417cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c72be565d2d447b9b76f57050146582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9f63bd56cb34940a972126eda1469b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa2a0aa5546f4c6c89b0a9057aa36ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b353497fe93a4e498c9757d3aa013a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eb60db730a34e3c87af01cfd2efa544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1acdf9dfe70944b0902d49cd761215f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4c600ca87a1485f969236795ebe620a",
              "IPY_MODEL_147dbc0d20a040bebfa94cbc8df04975",
              "IPY_MODEL_8dffb1fd721c4f89baf79c2d9ec87262"
            ],
            "layout": "IPY_MODEL_4b0f371251cb42db9c2f41d100801028"
          }
        },
        "e4c600ca87a1485f969236795ebe620a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d9b9bf28b684a18819d451e07df8f59",
            "placeholder": "​",
            "style": "IPY_MODEL_301b26fb016749bfaf2b7ad64571538b",
            "value": "Downloading: 100%"
          }
        },
        "147dbc0d20a040bebfa94cbc8df04975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b234b4720f04f928dad5bfc630a1705",
            "max": 891691430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cec718c5902464684565371aee0d9f4",
            "value": 891691430
          }
        },
        "8dffb1fd721c4f89baf79c2d9ec87262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_779387cc04e74124998a7de3fa87766d",
            "placeholder": "​",
            "style": "IPY_MODEL_2f7f26e3610b4b83b582311d288cef9e",
            "value": " 892M/892M [00:19&lt;00:00, 47.7MB/s]"
          }
        },
        "4b0f371251cb42db9c2f41d100801028": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d9b9bf28b684a18819d451e07df8f59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301b26fb016749bfaf2b7ad64571538b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b234b4720f04f928dad5bfc630a1705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cec718c5902464684565371aee0d9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "779387cc04e74124998a7de3fa87766d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f7f26e3610b4b83b582311d288cef9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7054a367a4044080822affa3556dc7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_559855afadf44131afeb3f60fdeb6339",
              "IPY_MODEL_28b8a1cf3b5b4cf39a7dc2eb0fa2581d",
              "IPY_MODEL_c60189992aab435b9e3cdfb3ea96941c"
            ],
            "layout": "IPY_MODEL_2d8d1af34de7435a9207a606007355e0"
          }
        },
        "559855afadf44131afeb3f60fdeb6339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54a694c180ab42b8aa4d985033d5b8a1",
            "placeholder": "​",
            "style": "IPY_MODEL_aaea056768b945e5a1dbc67c619dac2c",
            "value": "Downloading: 100%"
          }
        },
        "28b8a1cf3b5b4cf39a7dc2eb0fa2581d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aba11e177974175a6911c5fff9a5185",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6736471a20484b8986e0829e1e456a99",
            "value": 791656
          }
        },
        "c60189992aab435b9e3cdfb3ea96941c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b04d7c8a550740a4a5ceba60ecc39913",
            "placeholder": "​",
            "style": "IPY_MODEL_6698042f61524e85a25ddd4ba2c0c397",
            "value": " 792k/792k [00:00&lt;00:00, 2.10MB/s]"
          }
        },
        "2d8d1af34de7435a9207a606007355e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54a694c180ab42b8aa4d985033d5b8a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaea056768b945e5a1dbc67c619dac2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1aba11e177974175a6911c5fff9a5185": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6736471a20484b8986e0829e1e456a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b04d7c8a550740a4a5ceba60ecc39913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6698042f61524e85a25ddd4ba2c0c397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Start-up"
      ],
      "metadata": {
        "id": "zuaPjFtup46J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openprompt"
      ],
      "metadata": {
        "id": "Dbb4iXvHbyJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/sharvi24/event_detection/raw/main/maven_data.zip\n",
        "!unzip maven_data.zip"
      ],
      "metadata": {
        "id": "buoL02NKc_Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4-lmGxV_bcUe"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import tqdm\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from openprompt.data_utils import InputExample, InputFeatures\n",
        "from torch.utils.data._utils.collate import default_collate\n",
        "from openprompt.plms import load_plm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "K31s8_qwcWom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_div(a, b):\n",
        "    if b != 0:\n",
        "        return a / b \n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def evaluation(gold_labels, pred_labels, vocab):\n",
        "    inv_vocab = {v:k for k,v in vocab.items()}\n",
        "    result = {}\n",
        "    for label, idx in vocab.items():\n",
        "        if idx != 0:\n",
        "            result[label] = {\"prec\": 0.0, \"rec\": 0.0, \"f1\": 0.0}\n",
        "    \n",
        "    total_pred_num, total_gold_num, total_correct_num = 0.0, 0.0, 0.0\n",
        "\n",
        "    for i in range(len(gold_labels)):\n",
        "        pred_labels_i = pred_labels[i]\n",
        "        gold_labels_i = gold_labels[i]\n",
        "\n",
        "        for idx in gold_labels_i:\n",
        "            if idx != 0:\n",
        "                total_gold_num += 1\n",
        "                result[inv_vocab[idx]][\"rec\"] += 1\n",
        "        \n",
        "        for idx in pred_labels_i:\n",
        "            if idx != 0:\n",
        "                total_pred_num += 1\n",
        "                result[inv_vocab[idx]][\"prec\"] += 1\n",
        "        \n",
        "                if idx in gold_labels_i:\n",
        "                    total_correct_num += 1\n",
        "                    result[inv_vocab[idx]][\"f1\"] += 1\n",
        "    \n",
        "    for label in result:\n",
        "        counts = result[label]\n",
        "        counts[\"prec\"] = save_div(counts[\"f1\"], counts[\"prec\"])\n",
        "        counts[\"rec\"] = save_div(counts[\"f1\"], counts[\"rec\"])\n",
        "        counts[\"f1\"] = save_div(2*counts[\"prec\"]*counts[\"rec\"], counts[\"prec\"]+counts[\"rec\"])\n",
        "    \n",
        "    prec = save_div(total_correct_num, total_pred_num)\n",
        "    rec = save_div(total_correct_num, total_gold_num)\n",
        "    f1 = save_div(2*prec*rec, prec+rec)\n",
        "\n",
        "    return prec, rec, f1, result\n",
        "\n",
        "\n",
        "def load_data(input_dir):\n",
        "    data_items = []\n",
        "\n",
        "    with open(input_dir, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        item = json.loads(line)\n",
        "        data_items.append(item)\n",
        "    return data_items\n",
        "\n",
        "\n",
        "def get_vocab(train_dir, valid_dir):\n",
        "    train_data = load_data(train_dir)\n",
        "    valid_data = load_data(valid_dir)\n",
        "    vocab = {\"None\": 0}\n",
        "\n",
        "    for item in train_data:\n",
        "        for event in item[\"events\"]:\n",
        "            if event[-1] not in vocab:\n",
        "                vocab[event[-1]] = len(vocab)\n",
        "    \n",
        "    for item in valid_data:\n",
        "        for event in item[\"events\"]:\n",
        "            if event[-1] not in vocab:\n",
        "                vocab[event[-1]] = len(vocab)\n",
        "    \n",
        "    return vocab \n",
        "\n",
        "\n",
        "def process_data(input_dir, vocab):\n",
        "    items = load_data(input_dir)\n",
        "    output_items = []\n",
        "\n",
        "    for i,item in enumerate(items):\n",
        "        labels = []\n",
        "\n",
        "        for event in item[\"events\"]:\n",
        "            if vocab[event[-1]] not in labels:\n",
        "                labels.append(vocab[event[-1]])\n",
        "\n",
        "        if len(labels) == 0:\n",
        "            labels.append(0)\n",
        "\n",
        "        input_example = InputExample(text_a=\" \".join(item[\"tokens\"]), label=labels, guid=i)\n",
        "        output_items.append(input_example)\n",
        "\n",
        "    return output_items\n",
        "\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "    elem = batch[0]\n",
        "    return_dict = {}\n",
        "    for key in elem:\n",
        "        if key == \"encoded_tgt_text\":\n",
        "            return_dict[key] = [d[key] for d in batch]\n",
        "        else:\n",
        "            try:\n",
        "                return_dict[key] = default_collate([d[key] for d in batch])\n",
        "            except:\n",
        "                return_dict[key] = [d[key] for d in batch]\n",
        "\n",
        "    return InputFeatures(**return_dict)\n",
        "\n",
        "\n",
        "def convert_labels_to_list(labels):\n",
        "    label_list = []\n",
        "    for label in labels:\n",
        "        label_list.append(label.tolist().copy())\n",
        "    return label_list\n",
        "    \n",
        "\n",
        "def to_device(data, device):\n",
        "    for key in [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"loss_ids\"]:\n",
        "        data[key] = data[key].to(device)\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_plm():\n",
        "    # You may change the PLM you'd like to use. Currently it's t5-base.\n",
        "    return load_plm(\"t5\", \"t5-base\")\n",
        "\n",
        "def get_template():\n",
        "    # You may design your own template here. \n",
        "    return '{\"placeholder\":\"text_a\"} This text describes a {\"mask\"} event.'\n",
        "\n",
        "def get_verbalizer(vocab):\n",
        "    # Input: a dictionary for event types to indices: e.g.: {'None': 0, 'Catastrophe': 1, 'Causation': 2, 'Motion': 3, 'Hostile_encounter': 4, 'Process_start': 5, 'Attack': 6, 'Killing': 7, 'Conquering': 8, 'Social_event': 9, 'Competition': 10}\n",
        "\n",
        "    # Output: A 2-dim list. Verbalizers for each event type. Currently this function directly returns the lowercase for each event type name (and the performance is low). You may want to design your own verbalizers to improve the performance.\n",
        "\n",
        "    return [[label.lower()] for label in vocab]\n",
        "\n",
        "\n",
        "def loss_func(logits, labels):\n",
        "    \n",
        "    # INPUT: \n",
        "    ##  logits: a torch.Tensor of (batch_size, number_of_event_types) which is the output logits for each event type (none types are included).\n",
        "    ##  labels: a 2-dim List which denotes the ground-truth labels for each sentence in the batch. Note that there cound be multiple events, a single event, or no events for each sentence. \n",
        "    ##  For example, if labels == [[0], [2,3,4]] then the batch size is 2 and the first sentence has no events, and the second sentence has three events of indices 2,3 and 4.\n",
        "\n",
        "    ##  INSTRUCTIONS: In general, we want to maximize the logits of correct labels and minimize the logits with incorrect labels. You can implement your own loss function here or you can refer to what loss function is used in https://arxiv.org/pdf/2202.07615.pdf\n",
        "\n",
        "    ## OUTPUT:\n",
        "    ##   The output should be a pytorch scalar --- the loss.\n",
        "\n",
        "    ###  YOU NEED TO WRITE YOUR CODE HERE.  ###\n",
        "\n",
        "    \n",
        "    # total_loss = 0 \n",
        "    loss_list = []\n",
        "    for i, x in enumerate(labels):\n",
        "        whole_list = [1, 2, 3,4,5,6,7,8,9,10]\n",
        "        for zx in x:\n",
        "            if zx != 0:\n",
        "                whole_list.remove(zx)\n",
        "        if x[0] != 0:\n",
        "            loss_list.append(((logits[i][x].exp()/(logits[i][0].exp() + logits[i][x].exp())).mean()) + (logits[i][0].exp()/(logits[i][whole_list].exp().sum())))\n",
        "            #print((logits[i][x].exp()/(logits[i][0].exp() + logits[i][x].exp())).mean() + logits[i][0].exp()/logits[i][range(10)].exp().sum())\n",
        "        else:\n",
        "            loss_list.append(0 + logits[i][0].exp()/logits[i][whole_list].exp().sum())\n",
        "            #print(0 + logits[i][0].exp()/logits[i][range(10)].exp().sum())\n",
        "\n",
        "    loss = -torch.stack(loss_list).mean() \n",
        "    return loss\n",
        "    \n",
        "    # for b in range(batch_size):\n",
        "    #     L_pos, L_neg = 0, 0\n",
        "    #     null_exp_logit = logits[b][0].exp()\n",
        "    #     pos = 0\n",
        "    #     neg = null_exp_logit\n",
        "        \n",
        "    #     for i in range(1,11):\n",
        "    #         if i in labels[b]:\n",
        "    #             pos += torch.log(logits[b][i].exp()/logits[b][i].exp()+null_exp_logit)\n",
        "    #         else:\n",
        "    #             neg += torch.log(logits[b][i].exp())\n",
        "    #     L_pos = pos / len(labels[b])\n",
        "    #     L_neg = torch.log(null_exp_logit/neg)   \n",
        "    #     L_id = (L_pos + L_neg)/10\n",
        "    #     total_loss += L_id   \n",
        "\n",
        "    # loss = total_loss/batch_size\n",
        "    # return total_loss.clone()/batch_size\n",
        "  \n",
        "\n",
        "\n",
        "def predict(logits):\n",
        "    # INPUT: \n",
        "    ##  logits: a torch.Tensor of (batch_size, number_of_event_types) which is the output logits for each event type (none types are included).\n",
        "    # OUTPUT:\n",
        "    ##  a 2-dim list which has the same format with the \"labels\" in \"loss_func\" --- the predictions for all the sentences in the batch.\n",
        "    ##  For example, if predictions == [[0], [2,3,4]] then the batch size is 2, and we predict no events for the first sentence and three events (2,3,and 4) for the second sentence.\n",
        "\n",
        "    ##  INSTRUCTIONS: The most straight-forward way for prediction is to select out the indices with maximum of logits. Note that this is a multi-label classification problem, so each sentence could have multiple predicted event indices. Using what threshold for prediction is important here. You can also use the None event (index 0) as the threshold as what https://arxiv.org/pdf/2202.07615.pdf does.\n",
        "\n",
        "    ###  YOU NEED TO WRITE YOUR CODE HERE.  ###\n",
        "    print(logits.shape)\n",
        "    batch_size = logits.shape[0]\n",
        "    \n",
        "    predictions = []\n",
        "    for b in range(batch_size):\n",
        "        predictions.append([a for a in logits[b] if a>logits[b][0]])\n",
        "    \n",
        "    return predictions"
      ],
      "metadata": {
        "id": "pZSzz3l0oRca"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    test_dir = \"/content/maven_data/test.json\"\n",
        "    valid_dir = \"/content/maven_data/valid.json\"\n",
        "    train_dir = \"/content/maven_data/train.json\"\n",
        "\n",
        "    vocabulary = get_vocab(train_dir, valid_dir)\n",
        "    dataset = {\n",
        "        \"train\": process_data(train_dir, vocabulary),\n",
        "        \"validation\": process_data(valid_dir, vocabulary),\n",
        "        \"test\": process_data(test_dir, vocabulary)\n",
        "    }\n",
        "    print(vocabulary)\n",
        "    inv_vocabulary = {v:k for k,v in vocabulary.items()}\n",
        "    print(inv_vocabulary)\n",
        "    \n",
        "    from openprompt.prompts import ManualTemplate\n",
        "    plm, tokenizer, model_config, WrapperClass = get_plm()\n",
        "\n",
        "    template_text = get_template()\n",
        "    mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n",
        "\n",
        "    from openprompt import PromptDataLoader\n",
        "    train_dataloader = PromptDataLoader(\n",
        "        dataset=dataset[\"train\"], \n",
        "        template=mytemplate, \n",
        "        tokenizer=tokenizer,\n",
        "        tokenizer_wrapper_class=WrapperClass, \n",
        "        max_seq_length=256, \n",
        "        decoder_max_length=3,\n",
        "        batch_size=10,\n",
        "        shuffle=True, \n",
        "        teacher_forcing=False, \n",
        "        predict_eos_token=False,\n",
        "        truncate_method=\"head\"\n",
        "    )\n",
        "    train_dataloader.dataloader.collate_fn = my_collate_fn\n",
        "\n",
        "    validation_dataloader = PromptDataLoader(\n",
        "        dataset=dataset[\"validation\"], \n",
        "        template=mytemplate, \n",
        "        tokenizer=tokenizer,\n",
        "        tokenizer_wrapper_class=WrapperClass, \n",
        "        max_seq_length=256, \n",
        "        decoder_max_length=3,\n",
        "        batch_size=10,\n",
        "        shuffle=False, \n",
        "        teacher_forcing=False, \n",
        "        predict_eos_token=False,\n",
        "        truncate_method=\"head\"\n",
        "    )\n",
        "    validation_dataloader.dataloader.collate_fn = my_collate_fn\n",
        "\n",
        "    test_dataloader = PromptDataLoader(\n",
        "        dataset=dataset[\"test\"], \n",
        "        template=mytemplate, \n",
        "        tokenizer=tokenizer,\n",
        "        tokenizer_wrapper_class=WrapperClass, \n",
        "        max_seq_length=256, \n",
        "        decoder_max_length=3,\n",
        "        batch_size=10,\n",
        "        shuffle=False, \n",
        "        teacher_forcing=False, \n",
        "        predict_eos_token=False,\n",
        "        truncate_method=\"head\"\n",
        "    )\n",
        "    test_dataloader.dataloader.collate_fn = my_collate_fn\n",
        "\n",
        "    from openprompt.prompts import ManualVerbalizer\n",
        "    import torch\n",
        "\n",
        "    label_words = get_verbalizer(vocabulary)\n",
        "    # for example the verbalizer contains multiple label words in each class\n",
        "    myverbalizer = ManualVerbalizer(tokenizer, \n",
        "        num_classes=len(vocabulary),\n",
        "        label_words=label_words\n",
        "    )\n",
        "\n",
        "    from openprompt import PromptForClassification\n",
        "    use_cuda = True\n",
        "    prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
        "    if use_cuda:\n",
        "        prompt_model = prompt_model.cuda()\n",
        "    \n",
        "    from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    max_f1 = 0.0\n",
        "    max_patience, current_patience = 3, 0\n",
        "    if_exit = False\n",
        "\n",
        "    for epoch in range(10):\n",
        "        if if_exit:\n",
        "            break\n",
        "        tot_loss = 0.0\n",
        "        progress = tqdm.tqdm(total=len(train_dataloader), ncols=150, desc=\"Epoch: \"+str(epoch))\n",
        "        for step, inputs in enumerate(train_dataloader):\n",
        "            if if_exit:\n",
        "                break\n",
        "            if use_cuda:\n",
        "                inputs = to_device(inputs, device)\n",
        "            logits = prompt_model(inputs)\n",
        "            labels = inputs['label']\n",
        "            label_list = convert_labels_to_list(labels)\n",
        "            #loss = loss_func(logits, label_list)\n",
        "            #loss = loss_func(logits, label_list)\n",
        "            \n",
        "            loss.backward()\n",
        "            tot_loss += loss.item()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if step %100 ==99:\n",
        "                print(\"\\nStep {}, average loss: {}\".format(step, tot_loss/(step+1)), flush=True)\n",
        "\n",
        "                allpreds, alllabels = [], []\n",
        "                # Validation:\n",
        "                valid_progress = tqdm.tqdm(total=len(validation_dataloader), ncols=150, desc=\"Validation: \")\n",
        "                prompt_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for step, inputs in enumerate(validation_dataloader):\n",
        "                        if use_cuda:\n",
        "                            inputs = to_device(inputs, device)\n",
        "                        logits = prompt_model(inputs)\n",
        "                        labels = inputs['label']\n",
        "                        label_list = convert_labels_to_list(labels)\n",
        "                        pred_labels = predict(logits)\n",
        "\n",
        "                        alllabels.extend(label_list)\n",
        "                        allpreds.extend(pred_labels)\n",
        "                        valid_progress.update(1)\n",
        "\n",
        "                valid_progress.close()\n",
        "                prompt_model.train()\n",
        "                \n",
        "                p, r, f, total = evaluation(alllabels, allpreds, vocabulary)\n",
        "                print(\"F1-Score: \" + str(f))\n",
        "                with open(\"results.json\", 'w', encoding='utf-8') as f_out:\n",
        "                    f_out.write(json.dumps(total, indent=4))\n",
        "                if f > max_f1:\n",
        "                    max_f1 = f\n",
        "                    torch.save(prompt_model.state_dict(), \"./checkpoint_best.pt\")\n",
        "                    current_patience = 0\n",
        "                else:\n",
        "                    current_patience += 1\n",
        "                    if current_patience > max_patience:\n",
        "                        if_exit = True\n",
        "            \n",
        "                \n",
        "            progress.update(1)\n",
        "        progress.close()    "
      ],
      "metadata": {
        "id": "2n3IX21goHSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For debugging"
      ],
      "metadata": {
        "id": "5UyWaSWrrKQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = \"/content/maven_data/test.json\"\n",
        "valid_dir = \"/content/maven_data/valid.json\"\n",
        "train_dir = \"/content/maven_data/train.json\"\n",
        "\n",
        "vocabulary = get_vocab(train_dir, valid_dir)\n",
        "dataset = {\n",
        "    \"train\": process_data(train_dir, vocabulary),\n",
        "    \"validation\": process_data(valid_dir, vocabulary),\n",
        "    \"test\": process_data(test_dir, vocabulary)\n",
        "}\n",
        "print(vocabulary)\n",
        "inv_vocabulary = {v:k for k,v in vocabulary.items()}\n",
        "print(inv_vocabulary)\n",
        "\n",
        "from openprompt.prompts import ManualTemplate\n",
        "plm, tokenizer, model_config, WrapperClass = get_plm()\n",
        "\n",
        "template_text = get_template()\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n",
        "\n",
        "from openprompt import PromptDataLoader\n",
        "train_dataloader = PromptDataLoader(\n",
        "    dataset=dataset[\"train\"], \n",
        "    template=mytemplate, \n",
        "    tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, \n",
        "    max_seq_length=256, \n",
        "    decoder_max_length=3,\n",
        "    batch_size=10,\n",
        "    shuffle=True, \n",
        "    teacher_forcing=False, \n",
        "    predict_eos_token=False,\n",
        "    truncate_method=\"head\"\n",
        ")\n",
        "train_dataloader.dataloader.collate_fn = my_collate_fn\n",
        "\n",
        "validation_dataloader = PromptDataLoader(\n",
        "    dataset=dataset[\"validation\"], \n",
        "    template=mytemplate, \n",
        "    tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, \n",
        "    max_seq_length=256, \n",
        "    decoder_max_length=3,\n",
        "    batch_size=10,\n",
        "    shuffle=False, \n",
        "    teacher_forcing=False, \n",
        "    predict_eos_token=False,\n",
        "    truncate_method=\"head\"\n",
        ")\n",
        "validation_dataloader.dataloader.collate_fn = my_collate_fn\n",
        "\n",
        "test_dataloader = PromptDataLoader(\n",
        "    dataset=dataset[\"test\"], \n",
        "    template=mytemplate, \n",
        "    tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, \n",
        "    max_seq_length=256, \n",
        "    decoder_max_length=3,\n",
        "    batch_size=10,\n",
        "    shuffle=False, \n",
        "    teacher_forcing=False, \n",
        "    predict_eos_token=False,\n",
        "    truncate_method=\"head\"\n",
        ")\n",
        "test_dataloader.dataloader.collate_fn = my_collate_fn\n",
        "\n",
        "from openprompt.prompts import ManualVerbalizer\n",
        "import torch\n",
        "\n",
        "label_words = get_verbalizer(vocabulary)\n",
        "# for example the verbalizer contains multiple label words in each class\n",
        "myverbalizer = ManualVerbalizer(tokenizer, \n",
        "    num_classes=len(vocabulary),\n",
        "    label_words=label_words\n",
        ")\n",
        "\n",
        "from openprompt import PromptForClassification\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
        "if use_cuda:\n",
        "    prompt_model = prompt_model.cuda()\n",
        "\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "max_f1 = 0.0\n",
        "max_patience, current_patience = 3, 0\n",
        "if_exit = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719,
          "referenced_widgets": [
            "718b0a6731cb4717b78a3515cb1cffc0",
            "201f69abb3c0464c9732501450292c06",
            "c69c92defc93420ab6b16ca6086f1f4c",
            "6bce9241e7a849afa1afdf2476f8bbce",
            "c85fcefb0f664b0a82dfd862cae8a30c",
            "d2ce497d2df14dd8973d29b11c417cb1",
            "6c72be565d2d447b9b76f57050146582",
            "d9f63bd56cb34940a972126eda1469b8",
            "aa2a0aa5546f4c6c89b0a9057aa36ba1",
            "b353497fe93a4e498c9757d3aa013a1b",
            "7eb60db730a34e3c87af01cfd2efa544",
            "1acdf9dfe70944b0902d49cd761215f4",
            "e4c600ca87a1485f969236795ebe620a",
            "147dbc0d20a040bebfa94cbc8df04975",
            "8dffb1fd721c4f89baf79c2d9ec87262",
            "4b0f371251cb42db9c2f41d100801028",
            "8d9b9bf28b684a18819d451e07df8f59",
            "301b26fb016749bfaf2b7ad64571538b",
            "9b234b4720f04f928dad5bfc630a1705",
            "2cec718c5902464684565371aee0d9f4",
            "779387cc04e74124998a7de3fa87766d",
            "2f7f26e3610b4b83b582311d288cef9e",
            "7054a367a4044080822affa3556dc7f3",
            "559855afadf44131afeb3f60fdeb6339",
            "28b8a1cf3b5b4cf39a7dc2eb0fa2581d",
            "c60189992aab435b9e3cdfb3ea96941c",
            "2d8d1af34de7435a9207a606007355e0",
            "54a694c180ab42b8aa4d985033d5b8a1",
            "aaea056768b945e5a1dbc67c619dac2c",
            "1aba11e177974175a6911c5fff9a5185",
            "6736471a20484b8986e0829e1e456a99",
            "b04d7c8a550740a4a5ceba60ecc39913",
            "6698042f61524e85a25ddd4ba2c0c397"
          ]
        },
        "id": "2k2px2kXHMT_",
        "outputId": "903a5658-6077-4807-ffb2-0d921f2d2691"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'None': 0, 'Catastrophe': 1, 'Causation': 2, 'Motion': 3, 'Hostile_encounter': 4, 'Process_start': 5, 'Attack': 6, 'Killing': 7, 'Conquering': 8, 'Social_event': 9, 'Competition': 10}\n",
            "{0: 'None', 1: 'Catastrophe', 2: 'Causation', 3: 'Motion', 4: 'Hostile_encounter', 5: 'Process_start', 6: 'Attack', 7: 'Killing', 8: 'Conquering', 9: 'Social_event', 10: 'Competition'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "718b0a6731cb4717b78a3515cb1cffc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1acdf9dfe70944b0902d49cd761215f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7054a367a4044080822affa3556dc7f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n",
            "tokenizing: 5423it [00:08, 461.62it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (734 > 512). Running this sequence through the model will result in indexing errors\n",
            "tokenizing: 32431it [00:49, 661.03it/s]\n",
            "tokenizing: 8042it [00:11, 700.66it/s]\n",
            "tokenizing: 9400it [00:13, 693.64it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c94dbbf303ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mprompt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptForClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmytemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbalizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyverbalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze_plm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mprompt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "GHBRnhci2iFH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "    if if_exit:\n",
        "        break\n",
        "    tot_loss = 0.0\n",
        "    progress = tqdm.tqdm(total=len(train_dataloader), ncols=150, desc=\"Epoch: \"+str(epoch))\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if if_exit:\n",
        "            break\n",
        "        if use_cuda:\n",
        "            inputs = to_device(inputs, device)\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        label_list = convert_labels_to_list(labels)\n",
        "\n",
        "        sample_list = torch.zeros(20, 11, dtype=int)\n",
        "        for i,x in enumerate(labels):\n",
        "          sample_list[i][x] = 1\n",
        "        #print(\"sample_list\")\n",
        "        print(sample_list)\n",
        "        sample_list = sample_list.to(\"cuda:0\")\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss = loss_function(logits, sample_list.float())\n",
        "        loss.backward()\n",
        "        tot_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        #optimizer.zero_grad()\n",
        "\n",
        "        if step %100 ==99:\n",
        "            print(\"\\nStep {}, average loss: {}\".format(step, tot_loss/(step+1)), flush=True)\n",
        "\n",
        "            allpreds, alllabels = [], []\n",
        "            # Validation:\n",
        "            valid_progress = tqdm.tqdm(total=len(validation_dataloader), ncols=150, desc=\"Validation: \")\n",
        "            prompt_model.eval()\n",
        "            with torch.no_grad():\n",
        "                for step, inputs in enumerate(validation_dataloader):\n",
        "                    if use_cuda:\n",
        "                        inputs = to_device(inputs, device)\n",
        "                    logits = prompt_model(inputs)\n",
        "                    labels = inputs['label']\n",
        "                    label_list = convert_labels_to_list(labels)\n",
        "                    pred_labels = predict(logits)\n",
        "\n",
        "                    alllabels.extend(label_list)\n",
        "                    allpreds.extend(pred_labels)\n",
        "                    valid_progress.update(1)\n",
        "\n",
        "                valid_progress.close()\n",
        "                prompt_model.train()\n",
        "                \n",
        "                p, r, f, total = evaluation(alllabels, allpreds, vocabulary)\n",
        "                print(\"F1-Score: \" + str(f))\n",
        "                with open(\"results.json\", 'w', encoding='utf-8') as f_out:\n",
        "                    f_out.write(json.dumps(total, indent=4))\n",
        "                if f > max_f1:\n",
        "                    max_f1 = f\n",
        "                    torch.save(prompt_model.state_dict(), \"./checkpoint_best.pt\")\n",
        "                    current_patience = 0\n",
        "                else:\n",
        "                    current_patience += 1\n",
        "                    if current_patience > max_patience:\n",
        "                        if_exit = True\n",
        "            \n",
        "                \n",
        "            #loss.backward()\n",
        "            progress.update(1)\n",
        "        progress.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "yDGfDGKXTGKY",
        "outputId": "d0a5e15b-ec20-407a-964f-616b28ec45e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fe5d9166f291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mif_exit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtot_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Epoch: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'if_exit' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels = predict(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35l5UHFGMj9I",
        "outputId": "fe26249c-7533-42b3-8280-3330001b1a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # train_dir = \"./data/train.json\"\n",
        "    # valid_dir = \"./data/valid.json\"\n",
        "    # test_dir = \"./data/test.json\"\n",
        "\n",
        "    test_dir = \"/content/maven_data/test.json\"\n",
        "    valid_dir = \"/content/maven_data/valid.json\"\n",
        "    train_dir = \"/content/maven_data/train.json\"\n",
        "\n",
        "    vocabulary = get_vocab(train_dir, valid_dir)\n",
        "    dataset = {\n",
        "        \"train\": process_data(train_dir, vocabulary),\n",
        "        \"validation\": process_data(valid_dir, vocabulary),\n",
        "        \"test\": process_data(test_dir, vocabulary)\n",
        "    }\n",
        "    print(vocabulary)\n",
        "    inv_vocabulary = {v:k for k,v in vocabulary.items()}\n",
        "    print(inv_vocabulary)\n",
        "    \n",
        "    from openprompt.prompts import ManualTemplate\n",
        "    plm, tokenizer, model_config, WrapperClass = get_plm()\n",
        "\n",
        "    template_text = get_template()\n",
        "    mytemplate = ManualTemplate(tokenizer= tokenizer, text=template_text)\n",
        "\n",
        "    from openprompt import PromptDataLoader\n",
        "    train_dataloader = PromptDataLoader(\n",
        "        dataset=dataset[\"train\"], \n",
        "        template=mytemplate, \n",
        "        tokenizer=tokenizer,\n",
        "        tokenizer_wrapper_class=WrapperClass, \n",
        "        max_seq_length=256, \n",
        "        decoder_max_length=3,\n",
        "        batch_size=10,\n",
        "        shuffle=True, \n",
        "        teacher_forcing=False, \n",
        "        predict_eos_token=False,\n",
        "        truncate_method=\"head\"\n",
        "    )\n",
        "    train_dataloader.dataloader.collate_fn = my_collate_fn\n",
        "\n",
        "    validation_dataloader = PromptDataLoader(\n",
        "        dataset=dataset[\"validation\"], \n",
        "        template=mytemplate, \n",
        "        tokenizer=tokenizer,\n",
        "        tokenizer_wrapper_class=WrapperClass, \n",
        "        max_seq_length=256, \n",
        "        decoder_max_length=3,\n",
        "        batch_size=10,\n",
        "        shuffle=False, \n",
        "        teacher_forcing=False, \n",
        "        predict_eos_token=False,\n",
        "        truncate_method=\"head\"\n",
        "    )\n",
        "    validation_dataloader.dataloader.collate_fn = my_collate_fn\n",
        "\n",
        "    test_dataloader = PromptDataLoader(\n",
        "        dataset=dataset[\"test\"], \n",
        "        template=mytemplate, \n",
        "        tokenizer=tokenizer,\n",
        "        tokenizer_wrapper_class=WrapperClass, \n",
        "        max_seq_length=256, \n",
        "        decoder_max_length=3,\n",
        "        batch_size=10,\n",
        "        shuffle=False, \n",
        "        teacher_forcing=False, \n",
        "        predict_eos_token=False,\n",
        "        truncate_method=\"head\"\n",
        "    )\n",
        "    test_dataloader.dataloader.collate_fn = my_collate_fn\n",
        "\n",
        "    from openprompt.prompts import ManualVerbalizer\n",
        "    import torch\n",
        "\n",
        "    label_words = get_verbalizer(vocabulary)\n",
        "    # for example the verbalizer contains multiple label words in each class\n",
        "    myverbalizer = ManualVerbalizer(tokenizer, \n",
        "        num_classes=len(vocabulary),\n",
        "        label_words=label_words\n",
        "    )\n",
        "\n",
        "    from openprompt import PromptForClassification\n",
        "    use_cuda = True\n",
        "    prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
        "    if use_cuda:\n",
        "        prompt_model = prompt_model.cuda()\n",
        "    \n",
        "    from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    max_f1 = 0.0\n",
        "    max_patience, current_patience = 3, 0\n",
        "\n",
        "    for epoch in range(1):\n",
        "        progress = tqdm.tqdm(total=len(train_dataloader), ncols=150, desc=\"Epoch: \"+str(epoch))\n",
        "        \n",
        "        tot_loss = 0.0\n",
        "        for step, inputs in enumerate(train_dataloader):\n",
        "            if use_cuda:\n",
        "                inputs = to_device(inputs, device)\n",
        "            logits = prompt_model(inputs)\n",
        "            labels = inputs['label']\n",
        "            label_list = convert_labels_to_list(labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_func(logits, label_list)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tot_loss += loss.item()\n",
        "\n",
        "            if step %100 ==99:\n",
        "                print(\"\\nStep {}, average loss: {}\".format(step, tot_loss/(step+1)), flush=True)\n",
        "\n",
        "                allpreds, alllabels = [], []\n",
        "                # Validation:\n",
        "                valid_progress = tqdm.tqdm(total=len(validation_dataloader), ncols=150, desc=\"Validation: \")\n",
        "                for step, inputs in enumerate(validation_dataloader):\n",
        "                    if use_cuda:\n",
        "                        inputs = to_device(inputs, device)\n",
        "                    logits = prompt_model(inputs)\n",
        "                    labels = inputs['label']\n",
        "                    label_list = convert_labels_to_list(labels)\n",
        "                    pred_labels = predict(logits)\n",
        "\n",
        "                    alllabels.extend(pred_labels)\n",
        "                    allpreds.extend(label_list)\n",
        "                    valid_progress.update(1)\n",
        "\n",
        "                valid_progress.close()\n",
        "\n",
        "                p, r, f, total = evaluation(alllabels, allpreds, vocabulary)\n",
        "                print(\"F1-Score: \" + str(f))\n",
        "                with open(\"results.json\", 'w', encoding='utf-8') as f_out:\n",
        "                    f_out.write(json.dumps(total, indent=4))\n",
        "                if f > max_f1:\n",
        "                    max_f1 = f\n",
        "                    torch.save(prompt_model.state_dict(), \"./checkpoint_best.pt\")\n",
        "                    current_patience = 0\n",
        "                else:\n",
        "                    current_patience += 1\n",
        "                    if current_patience > max_patience:\n",
        "                        sys.exit(0)\n",
        "\n",
        "            progress.update(1)\n",
        "        progress.close()"
      ],
      "metadata": {
        "id": "JsDUp6MmcdtA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}